{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'NTIRE2024_ESR' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Amazingren/NTIRE2024_ESR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "41.20ms\n",
      "Max Memery:76.93[M]\n",
      "Height:128->512\n",
      "Width:128->512\n",
      "Parameters:753.34K\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numbers\n",
    "from basicsr.utils.registry import ARCH_REGISTRY\n",
    "from basicsr.archs.arch_util import trunc_normal_\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def img2windows(img, H_sp, W_sp):\n",
    "    \"\"\"\n",
    "    Input: Image (B, C, H, W)\n",
    "    Output: Window Partition (B', N, C)\n",
    "    \"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
    "    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp* W_sp, C)\n",
    "\n",
    "    return img_perm\n",
    "\n",
    "\n",
    "def windows2img(img_splits_hw, H_sp, W_sp, H, W):\n",
    "    \"\"\"\n",
    "    Input: Window Partition (B', N, C)\n",
    "    Output: Image (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n",
    "\n",
    "    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n",
    "    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.hidden_dims = hidden_features\n",
    "        self.in_dims = in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, n, _ = x.shape\n",
    "        self.N = n\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DynamicPosBias(nn.Module):\n",
    "    # The implementation builds on Crossformer code https://github.com/cheerss/CrossFormer/blob/main/models/crossformer.py\n",
    "    \"\"\" Dynamic Relative Position Bias.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        residual (bool):  If True, use residual strage to connect conv.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, residual):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.num_heads = num_heads\n",
    "        self.pos_dim = dim // 4\n",
    "        self.pos_proj = nn.Linear(2, self.pos_dim)\n",
    "        self.pos1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.pos_dim, self.pos_dim),\n",
    "        )\n",
    "        self.pos2 = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.pos_dim, self.pos_dim)\n",
    "        )\n",
    "        self.pos3 = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.pos_dim, self.num_heads)\n",
    "        )\n",
    "    def forward(self, biases):\n",
    "        self.l, self.c = biases.shape\n",
    "        if self.residual:\n",
    "            pos = self.pos_proj(biases) # 2Gh-1 * 2Gw-1, heads\n",
    "            pos = pos + self.pos1(pos)\n",
    "            pos = pos + self.pos2(pos)\n",
    "            pos = self.pos3(pos)\n",
    "        else:\n",
    "            pos = self.pos3(self.pos2(self.pos1(self.pos_proj(biases))))\n",
    "        return pos\n",
    "\n",
    "\n",
    "class Attention_regular(nn.Module):\n",
    "    \"\"\" Regular Rectangle-Window (regular-Rwin) self-attention with dynamic relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        resolution (int): Input resolution.\n",
    "        idx (int): The identix of V-Rwin and H-Rwin, 0 is H-Rwin, 1 is Vs-Rwin. (different order from Attention_axial)\n",
    "        split_size (tuple(int)): Height and Width of the regular rectangle window (regular-Rwin).\n",
    "        dim_out (int | None): The dimension of the attention output. Default: None\n",
    "        num_heads (int): Number of attention heads. Default: 6\n",
    "        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set\n",
    "        position_bias (bool): The dynamic relative position bias. Default: True\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, idx, split_size=[2,4], dim_out=None, num_heads=6, qk_scale=None, position_bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_out = dim_out or dim\n",
    "        self.split_size = split_size\n",
    "        self.num_heads = num_heads\n",
    "        self.idx = idx\n",
    "        self.position_bias = position_bias\n",
    "\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        if idx == 0:\n",
    "            H_sp, W_sp = self.split_size[0], self.split_size[1]\n",
    "        elif idx == 1:\n",
    "            W_sp, H_sp = self.split_size[0], self.split_size[1]\n",
    "        else:\n",
    "            print (\"ERROR MODE\", idx)\n",
    "            exit(0)\n",
    "        self.H_sp = H_sp\n",
    "        self.W_sp = W_sp\n",
    "\n",
    "        self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def im2win(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(-2,-1).contiguous().view(B, C, H, W)\n",
    "        x = img2windows(x, self.H_sp, self.W_sp)\n",
    "        x = x.reshape(-1, self.H_sp* self.W_sp, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, qkv, H, W, mask=None, rpi=None, rpe_biases=None):\n",
    "        \"\"\"\n",
    "        Input: qkv: (B, 3*L, C), H, W, mask: (B, N, N), N is the window size\n",
    "        Output: x (B, H, W, C)\n",
    "        \"\"\"\n",
    "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "\n",
    "        B, L, C = q.shape\n",
    "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
    "\n",
    "        self.N = L//(self.H_sp * self.W_sp)\n",
    "        # partition the q,k,v, image to window\n",
    "        q = self.im2win(q, H, W)\n",
    "        k = self.im2win(k, H, W)\n",
    "        v = self.im2win(v, H, W)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # B head N C @ B head C N --> B head N N\n",
    "\n",
    "        # calculate drpe\n",
    "        pos = self.pos(rpe_biases)\n",
    "        # select position bias\n",
    "        relative_position_bias = pos[rpi.view(-1)].view(\n",
    "            self.H_sp * self.W_sp, self.H_sp * self.W_sp, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        N = attn.shape[3]\n",
    "\n",
    "        # use mask for shift window\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        x = (attn @ v)\n",
    "        x = x.transpose(1, 2).reshape(-1, self.H_sp* self.W_sp, C)  # B head N N @ B head N C\n",
    "\n",
    "        # merge the window, window to image\n",
    "        x = windows2img(x, self.H_sp, self.W_sp, H, W)  # B H' W' C\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRWAB(nn.Module):\n",
    "    r\"\"\" Shift Rectangle Window Attention Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        split_size (int): Define the window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 split_size=(2,2),\n",
    "                 shift_size=(0,0),\n",
    "                 mlp_ratio=2.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.branch_num = 2\n",
    "        self.get_v = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1,groups=dim) # DW Conv\n",
    "\n",
    "        self.attns = nn.ModuleList([\n",
    "                Attention_regular(\n",
    "                    dim//2, idx = i,\n",
    "                    split_size=split_size, num_heads=num_heads//2, dim_out=dim//2,\n",
    "                    qk_scale=qk_scale, position_bias=True)\n",
    "                for i in range(self.branch_num)])\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x, x_size, params, attn_mask=NotImplementedError):\n",
    "        h, w = x_size\n",
    "        self.h,self.w = x_size\n",
    "\n",
    "        b, l, c = x.shape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        qkv = self.qkv(x).reshape(b, -1, 3, c).permute(2, 0, 1, 3) # 3, B, HW, C\n",
    "        v = qkv[2].transpose(-2,-1).contiguous().view(b, c, h, w)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            qkv = qkv.view(3, b, h, w, c)\n",
    "            # H-Shift\n",
    "            qkv_0 = torch.roll(qkv[:,:,:,:,:c//2], shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(2, 3))\n",
    "            qkv_0 = qkv_0.view(3, b, h*w, c//2)\n",
    "            # V-Shift\n",
    "            qkv_1 = torch.roll(qkv[:,:,:,:,c//2:], shifts=(-self.shift_size[1], -self.shift_size[0]), dims=(2, 3))\n",
    "            qkv_1 = qkv_1.view(3, b, h*w, c//2)\n",
    "\n",
    "            # H-Rwin\n",
    "            x1_shift = self.attns[0](qkv_0, h, w, mask=attn_mask[0], rpi=params['rpi_sa_h'], rpe_biases=params['biases_h'])\n",
    "            # V-Rwin\n",
    "            x2_shift = self.attns[1](qkv_1, h, w, mask=attn_mask[1], rpi=params['rpi_sa_v'], rpe_biases=params['biases_v'])\n",
    "\n",
    "            x1 = torch.roll(x1_shift, shifts=(self.shift_size[0], self.shift_size[1]), dims=(1, 2))\n",
    "            x2 = torch.roll(x2_shift, shifts=(self.shift_size[1], self.shift_size[0]), dims=(1, 2))\n",
    "            # Concat\n",
    "            attened_x = torch.cat([x1,x2], dim=-1)\n",
    "        else:\n",
    "            # H-Rwin\n",
    "            x1 = self.attns[0](qkv[:,:,:,:c//2], h, w, rpi=params['rpi_sa_h'], rpe_biases=params['biases_h'])\n",
    "            # V-Rwin\n",
    "            x2 = self.attns[1](qkv[:,:,:,c//2:], h, w, rpi=params['rpi_sa_v'], rpe_biases=params['biases_v'])\n",
    "            # Concat\n",
    "            attened_x = torch.cat([x1,x2], dim=-1)\n",
    "\n",
    "        attened_x = attened_x.view(b, -1, c).contiguous()\n",
    "\n",
    "        # Locality Complementary Module\n",
    "        lcm = self.get_v(v)\n",
    "        lcm = lcm.permute(0, 2, 3, 1).contiguous().view(b, -1, c)\n",
    "\n",
    "        attened_x = attened_x + lcm\n",
    "\n",
    "        attened_x = self.proj(attened_x)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + attened_x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class HFERB(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.mid_dim = dim//2\n",
    "        self.dim = dim\n",
    "        self.act = nn.GELU()\n",
    "        self.last_fc = nn.Conv2d(self.dim, self.dim, 1)\n",
    "\n",
    "        # High-frequency enhancement branch\n",
    "        self.fc = nn.Conv2d(self.mid_dim, self.mid_dim, 1)\n",
    "        self.max_pool = nn.MaxPool2d(3, 1, 1)\n",
    "\n",
    "        # Local feature extraction branch\n",
    "        self.conv = nn.Conv2d(self.mid_dim, self.mid_dim, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h, self.w = x.shape[2:]\n",
    "        short = x\n",
    "\n",
    "        # Local feature extraction branch\n",
    "        lfe = self.act(self.conv(x[:,:self.mid_dim,:,:]))\n",
    "\n",
    "        # High-frequency enhancement branch\n",
    "        hfe = self.act(self.fc(self.max_pool(x[:,self.mid_dim:,:,:])))\n",
    "\n",
    "        x = torch.cat([lfe, hfe], dim=1)\n",
    "        x = short + self.last_fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## High-frequency prior query inter attention layer\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias, train_size=(1, 3, 48, 48), base_size=(int(48 * 1.5), int(48 * 1.5))):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.train_size = train_size\n",
    "        self.base_size = base_size\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.dim = dim\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)\n",
    "        self.kv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)\n",
    "        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def _forward(self, q, kv):\n",
    "        k,v = kv.chunk(2, dim=1)\n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = self.softmax(attn)\n",
    "        out = (attn @ v)\n",
    "        return out\n",
    "\n",
    "    def forward(self, low, high):\n",
    "        self.h, self.w = low.shape[2:]\n",
    "\n",
    "        q = self.q_dwconv(self.q(high))\n",
    "        kv = self.kv_dwconv(self.kv(low))\n",
    "        out = self._forward(q, kv)\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=kv.shape[-2], w=kv.shape[-1])\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "##########################################################################\n",
    "## Improved feed-forward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "        self.hid_fea = hidden_features\n",
    "        self.dim = dim\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h, self.w = x.shape[2:]\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "##########################################################################\n",
    "class HFB(nn.Module):\n",
    "    r\"\"\" Hybrid Fusion Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ffn_expansion_factor (int): Define the window size.\n",
    "        bias (int): Shift size for SW-MSA.\n",
    "        LayerNorm_type (float): Ratio of mlp hidden dim to embedding dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
    "        super(HFB, self).__init__()\n",
    "\n",
    "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Attention(dim, num_heads, bias)\n",
    "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, low, high):\n",
    "        self.h, self.w = low.shape[2:]\n",
    "        x = low + self.attn(self.norm1(low), high)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class CRFB(nn.Module):\n",
    "    \"\"\" Cross-Refinement Fusion Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 split_size_0=7,\n",
    "                 split_size_1=7,\n",
    "                 mlp_ratio=2.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 norm_layer=nn.LayerNorm\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        # Shift Rectangle window attention blocks\n",
    "        self.srwa_blocks = nn.ModuleList([\n",
    "            SRWAB(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                split_size=[split_size_0,split_size_1],\n",
    "                shift_size=[0,0] if (i % 2 == 0) else [split_size_0//2, split_size_1//2],\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                norm_layer=norm_layer) for i in range(2*depth)\n",
    "        ])\n",
    "\n",
    "        # High frequency enhancement residual blocks\n",
    "        self.hfer_blocks = nn.ModuleList([\n",
    "                HFERB(dim)\n",
    "            for _ in range(depth)])\n",
    "\n",
    "        # Hybrid fusion blocks\n",
    "        self.hf_blocks = nn.ModuleList([\n",
    "            HFB(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                ffn_expansion_factor=2.66,\n",
    "                bias=False,\n",
    "                LayerNorm_type='WithBias') for i in range(depth)\n",
    "            ])\n",
    "\n",
    "    def forward(self, x, x_size, params):\n",
    "        b, c, h, w = x.shape\n",
    "        for i in range(self.depth):\n",
    "            low = x.permute(0, 2, 3, 1)\n",
    "            low = low.reshape(b, h*w, c)\n",
    "            low = self.srwa_blocks[2*i+1](self.srwa_blocks[2*i](low, x_size, params, params['attn_mask']), x_size, params, params['attn_mask'])\n",
    "            low = low.reshape(b, h, w, c)\n",
    "            low = low.permute(0, 3, 1, 2)\n",
    "            high = self.hfer_blocks[i](x)\n",
    "            x = self.hf_blocks[i](low, high)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class RCRFG(nn.Module):\n",
    "    \"\"\"Residual Cross-Refinement Fusion Group (RCRFG).\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=2.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 split_size_0 = 2,\n",
    "                 split_size_1 = 2,\n",
    "                 norm_layer=nn.LayerNorm\n",
    "                 ):\n",
    "        super(RCRFG, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.residual_group = CRFB(\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            split_size_0 = split_size_0,\n",
    "            split_size_1 = split_size_1,\n",
    "            norm_layer=norm_layer\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, x_size, params):\n",
    "        self.h, self.w = x_size\n",
    "        return self.conv(self.residual_group(x, x_size, params)) + x\n",
    "\n",
    "\n",
    "class UpsampleOneStep(nn.Sequential):\n",
    "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
    "       Used in lightweight SR to save parameters.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
    "        self.num_feat = num_feat\n",
    "        self.input_resolution = input_resolution\n",
    "        self.scale = scale\n",
    "        m = []\n",
    "        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "        super(UpsampleOneStep, self).__init__(*m)\n",
    "\n",
    "\n",
    "#@ARCH_REGISTRY.register()\n",
    "class CRAFT(nn.Module):\n",
    "    r\"\"\" Cross-Refinement Adaptive Fusion Transformer\n",
    "        Some codes are based on SwinIR.\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 2\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        upscale: Upscale factor. 2/3/4/\n",
    "        img_range: Image range. 1. or 255.\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=96,\n",
    "                 depths=(6, 6, 6, 6),\n",
    "                 num_heads=(6, 6, 6, 6),\n",
    "                 split_size_0 = 4,\n",
    "                 split_size_1 = 16,\n",
    "                 mlp_ratio=2.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 upscale=2,\n",
    "                 img_range=1.,\n",
    "                 upsampler='',\n",
    "                 resi_connection='1conv',\n",
    "                 **kwargs):\n",
    "        super(CRAFT, self).__init__()\n",
    "\n",
    "        self.split_size = (split_size_0, split_size_1)\n",
    "\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        self.num_feat = num_feat\n",
    "        self.num_out_ch = num_out_ch\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mean = torch.zeros(1, 1, 1, 1)\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "\n",
    "        # relative position index\n",
    "        self.calculate_rpi_v_sa()\n",
    "\n",
    "        # ------------------------- 1, shallow feature extraction ------------------------- #\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
    "\n",
    "        # ------------------------- 2, deep feature extraction ------------------------- #\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # build Residual Cross-Refinement Fusion Group\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = RCRFG(\n",
    "                dim=embed_dim,\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                split_size_0 = split_size_0,\n",
    "                split_size_1 = split_size_1,\n",
    "                norm_layer=norm_layer\n",
    "                )\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        self.norm = LayerNorm(self.num_features, 'with_bias')\n",
    "\n",
    "        # build the last conv layer in deep feature extraction\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        elif resi_connection == 'identity':\n",
    "            self.conv_after_body = nn.Identity()\n",
    "\n",
    "        # ------------------------- 3, high quality image reconstruction ------------------------- #\n",
    "        self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def calculate_rpi_v_sa(self):\n",
    "        # generate mother-set\n",
    "        H_sp, W_sp = self.split_size[0], self.split_size[1]\n",
    "        position_bias_h = torch.arange(1 - H_sp, H_sp)\n",
    "        position_bias_w = torch.arange(1 - W_sp, W_sp)\n",
    "        biases_h = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))\n",
    "        biases_h = biases_h.flatten(1).transpose(0, 1).contiguous().float()\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(H_sp)\n",
    "        coords_w = torch.arange(W_sp)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += H_sp - 1\n",
    "        relative_coords[:, :, 1] += W_sp - 1\n",
    "        relative_coords[:, :, 0] *= 2 * W_sp - 1\n",
    "        relative_position_index_h = relative_coords.sum(-1)\n",
    "\n",
    "\n",
    "        H_sp, W_sp = self.split_size[1], self.split_size[0]\n",
    "        position_bias_h = torch.arange(1 - H_sp, H_sp)\n",
    "        position_bias_w = torch.arange(1 - W_sp, W_sp)\n",
    "        biases_v = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))\n",
    "        biases_v = biases_v.flatten(1).transpose(0, 1).contiguous().float()\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(H_sp)\n",
    "        coords_w = torch.arange(W_sp)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += H_sp - 1\n",
    "        relative_coords[:, :, 1] += W_sp - 1\n",
    "        relative_coords[:, :, 0] *= 2 * W_sp - 1\n",
    "        relative_position_index_v = relative_coords.sum(-1)\n",
    "        self.register_buffer('relative_position_index_h', relative_position_index_h)\n",
    "        self.register_buffer('relative_position_index_v', relative_position_index_v)\n",
    "        self.register_buffer('biases_v', biases_v)\n",
    "        self.register_buffer('biases_h', biases_h)\n",
    "\n",
    "        return biases_v, biases_h\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x_size = (x.shape[2], x.shape[3])\n",
    "        params = {'attn_mask': (None, None), 'rpi_sa_h': self.relative_position_index_h, 'rpi_sa_v': self.relative_position_index_v, 'biases_v':self.biases_v, 'biases_h':self.biases_h}\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x_size, params)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h, self.w = x.shape[2:]\n",
    "        self.mean = self.mean.type_as(x)\n",
    "        x = (x - self.mean) * self.img_range\n",
    "\n",
    "        x = self.conv_first(x)\n",
    "        x = self.conv_after_body(self.forward_features(x)) + x\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = x / self.img_range + self.mean\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys \n",
    "    import os\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "\n",
    "    upscale = 4\n",
    "    window_size = 16\n",
    "    height = (512 // upscale // window_size) * window_size\n",
    "    width = (512 // upscale // window_size) * window_size\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    model = CRAFT(\n",
    "        upscale=upscale, img_size=(height, width), window_size=window_size,\n",
    "        img_range=1., depths=[2, 2, 2, 2],\n",
    "        embed_dim=48, num_heads=[6, 6, 6, 6], mlp_ratio=2,\n",
    "        split_size_0=4,\n",
    "        split_size_1=16\n",
    "    ).cuda()\n",
    "\n",
    "    params = sum(map(lambda x: x.numel(), model.parameters()))\n",
    "    results = dict()\n",
    "    results[f\"runtime\"] = []\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.randn((1, 3, height, width)).cuda()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            x_sr = model(x)\n",
    "        for _ in range(1):\n",
    "            start.record()\n",
    "            x_sr = model(x)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            results[f\"runtime\"].append(start.elapsed_time(end))  # milliseconds\n",
    "    print(x.shape)\n",
    "\n",
    "    print(\"{:.2f}ms\".format(sum(results[f\"runtime\"]) / len(results[f\"runtime\"])))\n",
    "    results[\"memory\"] = torch.cuda.max_memory_allocated(torch.cuda.current_device()) / 1024 ** 2\n",
    "    print(\"Max Memery:{:.2f}[M]\".format(results[\"memory\"]))\n",
    "    print(\"Height:{}->{}\\nWidth:{}->{}\\nParameters:{:.2f}K\".format(height, x_sr.shape[2], width, x_sr.shape[3], params / 1e3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "41.21ms\n",
      "Max Memery:76.93[M]\n",
      "Height:128->512\n",
      "Width:128->512\n",
      "Parameters:753.34K\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            x_sr = model(x)\n",
    "        for _ in range(1):\n",
    "            start.record()\n",
    "            x_sr = model(x)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            results[f\"runtime\"].append(start.elapsed_time(end))  # milliseconds\n",
    "print(x.shape)\n",
    "\n",
    "print(\"{:.2f}ms\".format(sum(results[f\"runtime\"]) / len(results[f\"runtime\"])))\n",
    "results[\"memory\"] = torch.cuda.max_memory_allocated(torch.cuda.current_device()) / 1024 ** 2\n",
    "print(\"Max Memery:{:.2f}[M]\".format(results[\"memory\"]))\n",
    "print(\"Height:{}->{}\\nWidth:{}->{}\\nParameters:{:.2f}K\".format(height, x_sr.shape[2], width, x_sr.shape[3], params / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    #Activations : 147.5871 [M]\n",
      "         #Conv2d : 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::sub encountered 18 time(s)\n",
      "Unsupported operator aten::mul encountered 313 time(s)\n",
      "Unsupported operator aten::add encountered 144 time(s)\n",
      "Unsupported operator aten::softmax encountered 40 time(s)\n",
      "Unsupported operator aten::gelu encountered 40 time(s)\n",
      "Unsupported operator aten::max_pool2d encountered 8 time(s)\n",
      "Unsupported operator aten::mul_ encountered 41 time(s)\n",
      "Unsupported operator aten::mean encountered 17 time(s)\n",
      "Unsupported operator aten::var encountered 17 time(s)\n",
      "Unsupported operator aten::sqrt encountered 17 time(s)\n",
      "Unsupported operator aten::div encountered 34 time(s)\n",
      "Unsupported operator aten::norm encountered 16 time(s)\n",
      "Unsupported operator aten::clamp_min encountered 16 time(s)\n",
      "Unsupported operator aten::expand_as encountered 16 time(s)\n",
      "Unsupported operator aten::pixel_shuffle encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           FLOPs : 55.8934 [G]\n",
      "         #Params : 753.3440 [K]\n",
      "         #Params : 0.7533 [M]\n"
     ]
    }
   ],
   "source": [
    "    from NTIRE2024_ESR.utils.model_summary import get_model_flops, get_model_activation\n",
    "    from NTIRE2024_ESR.models.team00_RLFN import RLFN_Prune\n",
    "    from fvcore.nn import FlopCountAnalysis\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CRAFT(\n",
    "        upscale=upscale, img_size=(height, width), window_size=window_size,\n",
    "        img_range=1., depths=[2, 2, 2, 2],\n",
    "        embed_dim=48, num_heads=[6, 6, 6, 6], mlp_ratio=2,\n",
    "        split_size_0=4,\n",
    "        split_size_1=16\n",
    "    ).to(device)\n",
    "    \n",
    "    input_dim = (3, 128, 128)  # set the input dimension\n",
    "    activations, num_conv = get_model_activation(model, input_dim)\n",
    "    activations = activations / 10 ** 6\n",
    "    print(\"{:>16s} : {:<.4f} [M]\".format(\"#Activations\", activations))\n",
    "    print(\"{:>16s} : {:<d}\".format(\"#Conv2d\", num_conv))\n",
    "\n",
    "    # The FLOPs calculation in previous NTIRE_ESR Challenge\n",
    "    # flops = get_model_flops(model, input_dim, False)\n",
    "    # flops = flops / 10 ** 9\n",
    "    # print(\"{:>16s} : {:<.4f} [G]\".format(\"FLOPs\", flops))\n",
    "\n",
    "    # fvcore is used in NTIRE2024_ESR for FLOPs calculation\n",
    "    input_fake = torch.rand(1, 3, 256, 256).to(device)\n",
    "    flops = FlopCountAnalysis(model, input_fake).total()\n",
    "    flops = flops/10**9\n",
    "    print(\"{:>16s} : {:<.4f} [G]\".format(\"FLOPs\", flops))\n",
    "\n",
    "    num_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n",
    "    num_parameters1 = num_parameters / 10 ** 3\n",
    "    print(\"{:>16s} : {:<.4f} [K]\".format(\"#Params\", num_parameters1))\n",
    "    num_parameters2 = num_parameters / 10 ** 6\n",
    "    print(\"{:>16s} : {:<.4f} [M]\".format(\"#Params\", num_parameters2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craft2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
